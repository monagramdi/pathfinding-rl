"""
Script pour debugger le comportement de l'agent en d√©tail.
"""
import numpy as np
from environment import MazeEnvironment
from maze_generator import MazeGenerator
from agent import QLearningAgent

def debug_agent_decision(agent, state):
    """Affiche les Q-values et la d√©cision de l'agent."""
    q_values = agent.q_table[state]
    action = agent.get_action(state, training=False)
    
    print(f"\n  √âtat: {state}")
    print(f"  Q-values: {q_values}")
    print(f"  Max Q: {np.max(q_values):.3f}")
    print(f"  Action choisie: {action} ({['Haut', 'Bas', 'Gauche', 'Droite'][action]})")
    
    # V√©rifier si toutes les Q-values sont √©gales
    if np.all(q_values == q_values[0]):
        print(f"  ‚ö†Ô∏è  √âTAT JAMAIS VU (toutes Q-values = {q_values[0]:.3f})")
    
    return action


def test_with_detailed_debug():
    """Test avec debug d√©taill√©."""
    print("="*60)
    print("DEBUG D√âTAILL√â DE L'AGENT")
    print("="*60)
    
    # Charger l'agent
    print("\n[1] Chargement de l'agent...")
    agent = QLearningAgent(n_actions=4)
    try:
        agent.load('saved_models/qlearning_agent.pkl')
        stats = agent.get_stats()
        print(f"‚úÖ Agent charg√©")
        print(f"   √âtats dans Q-table: {stats['n_states']}")
        print(f"   Epsilon: {stats['epsilon']}")
    except FileNotFoundError:
        print("‚ùå Pas d'agent sauvegard√©!")
        return
    
    # Cr√©er environnement
    print("\n[2] Cr√©ation du labyrinthe...")
    HEIGHT, WIDTH = 10, 10
    maze = MazeGenerator.simple_maze(HEIGHT, WIDTH, obstacle_ratio=0.2)
    start = (0, 0)
    goal = (HEIGHT - 1, WIDTH - 1)
    maze = MazeGenerator.ensure_path_exists(maze, start, goal)
    
    env = MazeEnvironment(maze, start, goal)
    print(f"‚úÖ Labyrinthe {HEIGHT}x{WIDTH} cr√©√©")
    
    # Afficher le labyrinthe
    print("\n[3] Labyrinthe:")
    env.render()
    
    # Test avec debug
    print("\n[4] Test de l'agent (max 30 steps pour debug):")
    print("="*60)
    
    state = env.reset()
    done = False
    steps = 0
    max_steps = 30
    
    action_counts = {0: 0, 1: 0, 2: 0, 3: 0}
    collision_count = 0
    last_states = []
    
    while not done and steps < max_steps:
        steps += 1
        print(f"\n--- STEP {steps} ---")
        
        # D√©cision de l'agent avec d√©tails
        action = debug_agent_decision(agent, state)
        action_counts[action] += 1
        
        # Ex√©cuter l'action
        next_state, reward, done, info = env.step(action)
        
        print(f"  R√©sultat:")
        print(f"    Nouvel √©tat: {next_state}")
        print(f"    Reward: {reward:.2f}")
        print(f"    Done: {done}")
        print(f"    Info: {info['reason']}")
        
        # D√©tecter les patterns probl√©matiques
        if info['reason'] == 'collision':
            collision_count += 1
            print(f"  üß± COLLISION #{collision_count}")
        
        if next_state == state:
            print(f"  ‚ö†Ô∏è  POSITION INCHANG√âE (collision ou bloqu√©)")
        
        # Tracker les derni√®res positions
        last_states.append(state)
        if len(last_states) > 5:
            last_states.pop(0)
            # D√©tecter boucle
            if len(set(last_states)) <= 2:
                print(f"  üîÑ BOUCLE D√âTECT√âE: {set(last_states)}")
        
        state = next_state
        
        # Afficher le labyrinthe tous les 5 steps
        if steps % 5 == 0:
            env.render()
    
    # R√©sum√©
    print("\n" + "="*60)
    print("R√âSUM√â DU DEBUG")
    print("="*60)
    print(f"Steps effectu√©s: {steps}")
    print(f"Raison d'arr√™t: {info.get('reason', 'max_steps')}")
    print(f"Position finale: {state}")
    print(f"Goal: {goal}")
    print(f"\nDistribution des actions:")
    for action, count in action_counts.items():
        pct = (count / steps * 100) if steps > 0 else 0
        action_name = ['Haut', 'Bas', 'Gauche', 'Droite'][action]
        print(f"  {action_name}: {count} fois ({pct:.1f}%)")
    print(f"\nCollisions totales: {collision_count}")
    
    # Diagnostic
    print("\n" + "="*60)
    print("DIAGNOSTIC")
    print("="*60)
    
    # Probl√®me 1: Une action dominante
    max_action_count = max(action_counts.values())
    if max_action_count > steps * 0.7:
        dominant_action = max(action_counts, key=action_counts.get)
        print(f"‚ùå PROBL√àME: Action {dominant_action} ({['Haut', 'Bas', 'Gauche', 'Droite'][dominant_action]}) utilis√©e {max_action_count}/{steps} fois")
        print(f"   ‚Üí L'agent r√©p√®te la m√™me action en boucle")
        print(f"   ‚Üí Solutions possibles:")
        print(f"      1. Augmenter epsilon pendant le test (test_epsilon=0.2)")
        print(f"      2. R√©entra√Æner avec plus d'exploration")
        print(f"      3. V√©rifier que les Q-values sont bien apprises")
    
    # Probl√®me 2: Trop de collisions
    if collision_count > steps * 0.5:
        print(f"‚ùå PROBL√àME: Trop de collisions ({collision_count}/{steps})")
        print(f"   ‚Üí L'agent n'a pas appris √† √©viter les obstacles")
        print(f"   ‚Üí Solutions:")
        print(f"      1. R√©entra√Æner avec plus d'√©pisodes (5000+)")
        print(f"      2. Augmenter la p√©nalit√© de collision dans environment.py")
    
    # Probl√®me 3: √âtats jamais vus
    print(f"\nüìä V√©rification de la couverture de la Q-table:")
    states_checked = 0
    states_learned = 0
    for i in range(HEIGHT):
        for j in range(WIDTH):
            if maze[i, j] == 0:  # Case libre
                states_checked += 1
                q_vals = agent.q_table[(i, j)]
                if not np.all(q_vals == 0):
                    states_learned += 1
    
    coverage = (states_learned / states_checked * 100) if states_checked > 0 else 0
    print(f"  √âtats libres dans le maze: {states_checked}")
    print(f"  √âtats avec Q-values > 0: {states_learned}")
    print(f"  Couverture: {coverage:.1f}%")
    
    if coverage < 50:
        print(f"‚ùå PROBL√àME: Couverture trop faible ({coverage:.1f}%)")
        print(f"   ‚Üí L'agent n'a pas explor√© assez d'√©tats")
        print(f"   ‚Üí Solution: R√©entra√Æner avec plus d'√©pisodes")
    
    # Recommandations finales
    print("\n" + "="*60)
    print("RECOMMANDATIONS")
    print("="*60)
    
    if coverage < 50:
        print("üî¥ PRIORIT√â 1: R√©entra√Æner l'agent")
        print("   python train.py")
        print("   Avec N_EPISODES = 10000")
    elif collision_count > steps * 0.5:
        print("üü† PRIORIT√â 2: Am√©liorer l'apprentissage des obstacles")
        print("   1. Augmenter epsilon_decay √† 0.998 dans train.py")
        print("   2. R√©entra√Æner avec 5000 √©pisodes")
    else:
        print("üü° Ajouter de l'exploration pendant le test")
        print("   Dans test.py, utiliser test_epsilon=0.2")
        print("   results = test_agent(env, agent, test_epsilon=0.2)")


if __name__ == "__main__":
    test_with_detailed_debug()