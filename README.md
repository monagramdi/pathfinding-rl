# üéØ Pathfinding avec Reinforcement Learning

Projet d'apprentissage par renforcement (Q-Learning) pour la r√©solution de probl√®mes de pathfinding dans des labyrinthes.

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## üìã Description

Ce projet impl√©mente un agent d'apprentissage par renforcement capable de naviguer dans des labyrinthes pour trouver le chemin optimal entre un point de d√©part et une destination. L'agent utilise l'algorithme **Q-Learning** pour apprendre une politique optimale.

### üéØ Objectifs

- ‚úÖ Impl√©mentation compl√®te de Q-Learning (tabular)
- ‚úÖ Comparaison avec algorithmes classiques (A*, BFS, Dijkstra)
- ‚úÖ Visualisations d√©taill√©es des r√©sultats
- ‚úÖ Gestion d'environnements avec obstacles

## üöÄ Installation

### Pr√©requis

- Python 3.9 ou sup√©rieur
- pip

### √âtapes d'installation

```bash
# Cloner le repository
git clone https://github.com/votre-username/pathfinding-rl.git
cd pathfinding-rl

# Cr√©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

## üéÆ Utilisation

### 1Ô∏è‚É£ Entra√Ænement de l'agent

Entra√Æner l'agent sur plusieurs labyrinthes diff√©rents :

```bash
python train.py
```

**Param√®tre configurable** (dans le script) :
- `N_EPISODES` : Nombre d'√©pisodes d'entra√Ænement (d√©faut: 10000)

**R√©sultats attendus** :
- Taux de succ√®s : 60-80%
- Dur√©e : 30-60 minutes
- Fichier sauvegard√© : `saved_models/qlearning_agent.pkl`

### 2Ô∏è‚É£ Test de l'agent

Tester l'agent entra√Æn√© sur le m√™me labyrinthe :

```bash
python test.py
```

Affiche :
- Animation en temps r√©el de la navigation
- Statistiques de performance
- Visualisation du chemin trouv√©
- √âvaluation sur 100 √©pisodes

### 3Ô∏è‚É£ Comparaison avec algorithmes classiques

Comparer les performances de l'agent RL avec A*, BFS et Dijkstra :

```bash
python compare.py
```

G√©n√®re :
- Tableau comparatif des performances
- Visualisation des diff√©rents chemins
- Analyse de la qualit√© des solutions



## üìä Structure du Projet

```
pathfinding_rl/
‚îÇ
‚îú‚îÄ‚îÄ environment.py          # Environnement du labyrinthe (actions, r√©compenses)
‚îú‚îÄ‚îÄ maze_generator.py       # G√©n√©ration de labyrinthes (plusieurs m√©thodes)
‚îú‚îÄ‚îÄ agent.py               # Agent Q-Learning et SARSA
‚îú‚îÄ‚îÄ train.py               # Script d'entra√Ænement simple (1 labyrinthe)
‚îú‚îÄ‚îÄ test.py                # Script de test et √©valuation
‚îú‚îÄ‚îÄ compare.py             # Comparaison avec algorithmes classiques
‚îú‚îÄ‚îÄ visualizer.py          # Outils de visualisation avanc√©s
‚îú‚îÄ‚îÄ utils.py               # Fonctions utilitaires (A*, BFS, Dijkstra)
‚îú‚îÄ‚îÄ saved_models/          # Mod√®les sauvegard√©s (.pkl)
‚îú‚îÄ‚îÄ results/               # Graphiques et r√©sultats (.png)
‚îú‚îÄ‚îÄ logs/                  # Logs d'entra√Ænement
‚îú‚îÄ‚îÄ mazes/                 # Labyrinthes sauvegard√©s
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python
‚îî‚îÄ‚îÄ README.md             # Documentation
```

## üß† Algorithmes Impl√©ment√©s

### Reinforcement Learning
- **Q-Learning** (tabular) : Apprentissage off-policy
- **SARSA** : Variante on-policy (bonus)

### Algorithmes Classiques (baseline)
- **A*** : Optimal avec heuristique
- **BFS** : Breadth-First Search
- **Dijkstra** : Plus court chemin

## üìà R√©sultats

### Performance de l'agent Q-Learning

Apr√®s entra√Ænement sur 50 labyrinthes diff√©rents (10000 √©pisodes) :

| M√©trique | Valeur |
|----------|--------|
| Taux de succ√®s | 80-90% |
| Steps moyens | 20-30 |


### Comparaison des algorithmes

| Algorithme | Steps | Temps | Optimal |
|------------|-------|-------|---------|
| A* | 19 | 0.5ms | ‚úÖ |
| BFS | 19 | 1.2ms | ‚úÖ |
| Dijkstra | 19 | 0.8ms | ‚úÖ |
| RL Agent | 22-25 | 0.3ms | ~90% |

## üé® Visualisations

Le projet g√©n√®re automatiquement :

1. **Courbes d'apprentissage** : Rewards, steps, taux de succ√®s
2. **Visualisation des chemins** : Comparaison visuelle des solutions
3. **Q-values** : Heatmap des valeurs apprises (bonus)
4. **Politique** : Directions pr√©f√©r√©es par √©tat (bonus)

Exemples dans `results/` :
- `training_stats_multi.png`
- `path_visualization.png`
- `algorithms_comparison.png`

## ‚öôÔ∏è Configuration Technique

### Environnement

- **√âtat** : Position (x, y) dans la grille
- **Actions** : Haut (0), Bas (1), Gauche (2), Droite (3)
- **R√©compenses** :
  - Goal atteint : +100
  - Collision mur : -10
  - Step normal : -1
  - Se rapprocher : +0.5 (reward shaping)
  - Case revisit√©e : -0.5 (p√©nalit√©)

### Hyperparam√®tres Q-Learning

```python
learning_rate (Œ±) = 0.1        # Taux d'apprentissage
discount_factor (Œ≥) = 0.95     # Importance du futur
epsilon (Œµ) = 1.0 ‚Üí 0.1        # Exploration ‚Üí Exploitation
epsilon_decay = 0.9995         # D√©croissance de Œµ
```

### Conditions d'arr√™t

Un √©pisode se termine si :
- ‚úÖ Goal atteint (succ√®s)
- ‚è±Ô∏è Limite de steps atteinte (500 max)
- üîÑ Boucle d√©tect√©e (m√™me position > 4 fois)
- üß± Trop de collisions cons√©cutives (> 10)

## üîß Personnalisation

### Modifier la taille du labyrinthe

Dans `train.py` :
```python
HEIGHT, WIDTH = 15, 15  # Au lieu de 10x10
```

### Changer les hyperparam√®tres

Dans `train.py` :
```python
agent = QLearningAgent(
    learning_rate=0.15,      # Plus rapide
    epsilon_decay=0.999,     # Plus d'exploration
    epsilon_min=0.05         # Minimum d'exploration
)
```

## üêõ Troubleshooting

### L'agent ne trouve jamais le goal

**Probl√®me** : Pas assez entra√Æn√© ou mauvais hyperparam√®tres

**Solution** :
```bash
# Augmenter les √©pisodes
N_EPISODES = 15000

# Ou r√©duire la difficult√©
OBSTACLE_RATIO = 0.1  # Moins d'obstacles
```

### Agent bloqu√© en boucle

**Probl√®me** : Pas assez d'exploration pendant le test

**Solution** : Dans `test.py`, augmenter `test_epsilon`
```python
test_agent(env, agent, test_epsilon=0.2)  # 20% exploration
```


## üìö Concepts Cl√©s

### Q-Learning

Mise √† jour de la Q-table selon l'√©quation de Bellman :

```
Q(s,a) = Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]
```

O√π :
- `s` : √©tat actuel
- `a` : action choisie
- `r` : r√©compense re√ßue
- `s'` : nouvel √©tat
- `Œ±` : learning rate
- `Œ≥` : discount factor

### Exploration vs Exploitation

**Epsilon-greedy** :
- Avec probabilit√© `Œµ` : action al√©atoire (exploration)
- Avec probabilit√© `1-Œµ` : meilleure action (exploitation)
- `Œµ` d√©cro√Æt au fil du temps : `Œµ = Œµ √ó decay`

### Reward Shaping

Technique pour guider l'apprentissage :
- R√©compenses interm√©diaires pour se rapprocher du goal
- P√©nalit√©s pour revisiter des cases
- P√©nalit√©s fortes pour collisions r√©p√©t√©es

## üìù Am√©liorations Possibles

- [ ] Impl√©mentation Deep Q-Network (DQN) pour grands labyrinthes
- [ ] Support de labyrinthes 3D
- [ ] Multi-agents coop√©ratifs
- [ ] Environnements dynamiques (obstacles mobiles)
- [ ] Interface graphique interactive (Pygame)
- [ ] Apprentissage par imitation (Imitation Learning)


## üë§ Auteur

Mona Gramdi - Projet Pathfinding RL